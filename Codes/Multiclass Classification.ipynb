{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, r2_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41740, 18])\n",
      "torch.Size([41740])\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = np.load('Classified_Data_Var.npz')\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X = torch.Tensor(data['features'])\n",
    "y = torch.Tensor(data['labels']).long()\n",
    "\n",
    "print(X.shape)  # Should output: (41740, 9, 9)\n",
    "print(y.shape)  # Should output: (41740,)\n",
    "\n",
    "# Split the data into training+validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training+validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 0.25 of the train_val set, i.e., 20% of the original data\n",
    "\n",
    "# If you need to flatten the 9x9 matrices into 81-element vectors, do this:\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Standardize the training and validation data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the numpy arrays back to tensors\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_val = torch.Tensor(X_val)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.Tensor(y_train).long()\n",
    "y_val = torch.Tensor(y_val).long()\n",
    "y_test = torch.Tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embed size needs to be divisible by number of heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Split the embedding into `num_heads` different pieces\n",
    "        values = self.values(values).view(values.shape[0], values.shape[1], self.num_heads, self.head_dim)\n",
    "        keys = self.keys(keys).view(keys.shape[0], keys.shape[1], self.num_heads, self.head_dim)\n",
    "        queries = self.queries(query).view(query.shape[0], query.shape[1], self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose for the dot product matrix multiplication to be computable\n",
    "        values = values.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = F.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
    "        out = out.transpose(1, 2).contiguous().view(values.shape[0], -1, self.num_heads * self.head_dim)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Example feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, embed_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        \n",
    "        x = self.norm1(attention + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.transformer_block = TransformerBlock(embed_size, num_heads)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_size * 2, 1024)  # Assuming a flatten operation before this layer\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 240)\n",
    "        self.fc4 = nn.Linear(240, 168)\n",
    "        self.fc_output = nn.Linear(168, num_classes)  # 最终输出类别数量\n",
    "\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(240)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        N = x.shape[0]\n",
    "        x = x.view(N, 2, 9)  # Reshape to (batch_size, sequence_length, embed_size)\n",
    "        # Apply transformer block\n",
    "        x = self.transformer_block(x, x, x, mask)\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(N, -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(F.relu(self.bn(self.fc3(x))))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc_output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "embed_size = 9  # Size of each input token's embedding\n",
    "num_heads = 3  # Number of heads in the multi-head attention mechanism\n",
    "num_classes = 2  # Number of output classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20000], Loss: 0.6198, Validation Loss: 0.7244, Validation Accuracy: 0.6554\n",
      "Epoch [20/20000], Loss: 0.6114, Validation Loss: 0.6206, Validation Accuracy: 0.6641\n",
      "Epoch [30/20000], Loss: 0.6076, Validation Loss: 0.6123, Validation Accuracy: 0.6681\n",
      "Epoch [40/20000], Loss: 0.6043, Validation Loss: 0.6135, Validation Accuracy: 0.6640\n",
      "Epoch [50/20000], Loss: 0.6016, Validation Loss: 0.6131, Validation Accuracy: 0.6681\n",
      "Epoch [60/20000], Loss: 0.5979, Validation Loss: 0.6140, Validation Accuracy: 0.6670\n",
      "Epoch [70/20000], Loss: 0.5978, Validation Loss: 0.6110, Validation Accuracy: 0.6700\n",
      "Epoch [80/20000], Loss: 0.5900, Validation Loss: 0.6100, Validation Accuracy: 0.6723\n",
      "Epoch [90/20000], Loss: 0.5874, Validation Loss: 0.6095, Validation Accuracy: 0.6688\n",
      "Epoch [100/20000], Loss: 0.5811, Validation Loss: 0.6098, Validation Accuracy: 0.6691\n",
      "Epoch [110/20000], Loss: 0.5843, Validation Loss: 0.6100, Validation Accuracy: 0.6699\n",
      "Epoch [120/20000], Loss: 0.5746, Validation Loss: 0.6125, Validation Accuracy: 0.6699\n",
      "Epoch [130/20000], Loss: 0.5700, Validation Loss: 0.6158, Validation Accuracy: 0.6658\n",
      "Epoch [140/20000], Loss: 0.5600, Validation Loss: 0.6188, Validation Accuracy: 0.6696\n",
      "Epoch [150/20000], Loss: 0.5752, Validation Loss: 0.6328, Validation Accuracy: 0.6548\n",
      "Epoch [160/20000], Loss: 0.5442, Validation Loss: 0.6228, Validation Accuracy: 0.6635\n",
      "Epoch [170/20000], Loss: 0.5503, Validation Loss: 0.6216, Validation Accuracy: 0.6670\n",
      "Epoch [180/20000], Loss: 0.5366, Validation Loss: 0.6333, Validation Accuracy: 0.6645\n",
      "Epoch [190/20000], Loss: 0.5309, Validation Loss: 0.6588, Validation Accuracy: 0.6555\n",
      "Epoch [200/20000], Loss: 0.5102, Validation Loss: 0.6714, Validation Accuracy: 0.6564\n",
      "Epoch [210/20000], Loss: 0.5145, Validation Loss: 0.6709, Validation Accuracy: 0.6592\n",
      "Epoch [220/20000], Loss: 0.4821, Validation Loss: 0.7042, Validation Accuracy: 0.6526\n",
      "Epoch [230/20000], Loss: 0.5004, Validation Loss: 0.6663, Validation Accuracy: 0.6579\n",
      "Epoch [240/20000], Loss: 0.4723, Validation Loss: 0.7371, Validation Accuracy: 0.6556\n",
      "Epoch [250/20000], Loss: 0.4579, Validation Loss: 0.7447, Validation Accuracy: 0.6443\n",
      "Epoch [260/20000], Loss: 0.4506, Validation Loss: 0.7895, Validation Accuracy: 0.6482\n",
      "Epoch [270/20000], Loss: 0.4342, Validation Loss: 0.7799, Validation Accuracy: 0.6483\n",
      "Epoch [280/20000], Loss: 0.4246, Validation Loss: 0.8230, Validation Accuracy: 0.6490\n",
      "Epoch [290/20000], Loss: 0.4128, Validation Loss: 0.8673, Validation Accuracy: 0.6489\n",
      "Epoch [300/20000], Loss: 0.4180, Validation Loss: 0.8315, Validation Accuracy: 0.6412\n",
      "Epoch [310/20000], Loss: 0.3846, Validation Loss: 0.9340, Validation Accuracy: 0.6460\n",
      "Epoch [320/20000], Loss: 0.3954, Validation Loss: 0.8970, Validation Accuracy: 0.6447\n",
      "Epoch [330/20000], Loss: 0.3689, Validation Loss: 1.0537, Validation Accuracy: 0.6496\n",
      "Epoch [340/20000], Loss: 0.3550, Validation Loss: 0.9533, Validation Accuracy: 0.6366\n",
      "Epoch [350/20000], Loss: 0.3434, Validation Loss: 0.9987, Validation Accuracy: 0.6533\n",
      "Epoch [360/20000], Loss: 0.3531, Validation Loss: 0.9792, Validation Accuracy: 0.6388\n",
      "Epoch [370/20000], Loss: 0.3406, Validation Loss: 1.1137, Validation Accuracy: 0.6494\n",
      "Epoch [380/20000], Loss: 0.3205, Validation Loss: 1.0564, Validation Accuracy: 0.6441\n",
      "Epoch [390/20000], Loss: 0.3308, Validation Loss: 1.0580, Validation Accuracy: 0.6384\n",
      "Epoch [400/20000], Loss: 0.3034, Validation Loss: 1.0796, Validation Accuracy: 0.6503\n",
      "Epoch [410/20000], Loss: 0.3046, Validation Loss: 1.0513, Validation Accuracy: 0.6399\n",
      "Epoch [420/20000], Loss: 0.3036, Validation Loss: 1.2802, Validation Accuracy: 0.6424\n",
      "Epoch [430/20000], Loss: 0.2720, Validation Loss: 1.1451, Validation Accuracy: 0.6385\n",
      "Epoch [440/20000], Loss: 0.2793, Validation Loss: 1.1447, Validation Accuracy: 0.6384\n",
      "Epoch [450/20000], Loss: 0.2739, Validation Loss: 1.1459, Validation Accuracy: 0.6381\n",
      "Epoch [460/20000], Loss: 0.2621, Validation Loss: 1.2590, Validation Accuracy: 0.6385\n",
      "Epoch [470/20000], Loss: 0.2466, Validation Loss: 1.2895, Validation Accuracy: 0.6345\n",
      "Epoch [480/20000], Loss: 0.2481, Validation Loss: 1.2603, Validation Accuracy: 0.6406\n",
      "Epoch [490/20000], Loss: 0.2483, Validation Loss: 1.3061, Validation Accuracy: 0.6376\n",
      "Epoch [500/20000], Loss: 0.2308, Validation Loss: 1.3624, Validation Accuracy: 0.6364\n",
      "Epoch [510/20000], Loss: 0.2450, Validation Loss: 1.2397, Validation Accuracy: 0.6363\n",
      "Epoch [520/20000], Loss: 0.2423, Validation Loss: 1.2612, Validation Accuracy: 0.6300\n",
      "Epoch [530/20000], Loss: 0.2280, Validation Loss: 1.2763, Validation Accuracy: 0.6223\n",
      "Epoch [540/20000], Loss: 0.2146, Validation Loss: 1.3322, Validation Accuracy: 0.6354\n",
      "Epoch [550/20000], Loss: 0.2261, Validation Loss: 1.3656, Validation Accuracy: 0.6210\n",
      "Epoch [560/20000], Loss: 0.2095, Validation Loss: 1.5634, Validation Accuracy: 0.6390\n",
      "Epoch [570/20000], Loss: 0.2221, Validation Loss: 1.1901, Validation Accuracy: 0.6334\n",
      "Epoch [580/20000], Loss: 0.2094, Validation Loss: 1.4365, Validation Accuracy: 0.6439\n",
      "Epoch [590/20000], Loss: 0.1981, Validation Loss: 1.5547, Validation Accuracy: 0.6431\n",
      "Epoch [600/20000], Loss: 0.1877, Validation Loss: 1.4127, Validation Accuracy: 0.6357\n",
      "Epoch [610/20000], Loss: 0.1916, Validation Loss: 1.4015, Validation Accuracy: 0.6307\n",
      "Epoch [620/20000], Loss: 0.1796, Validation Loss: 1.4897, Validation Accuracy: 0.6303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 59\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 注意：在实际使用之前，你需要定义X_train, y_train, X_val, y_val，并确保它们是正确的形状和类型，并且已经转移到了GPU\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# X_train, y_train, X_val, y_val = ...\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, X_train, y_train, X_val, y_val, n_epochs, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清零梯度\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x))\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_output(x)\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 检查CUDA是否可用并设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net(embed_size, num_heads, num_classes)\n",
    "# 将模型移到设备上\n",
    "model = net.to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# 学习率调度器\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.1, patience=10)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, criterion, optimizer, scheduler, X_train, y_train, X_val, y_val, n_epochs, device):\n",
    "    # 确保输入数据在正确的设备上\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 验证模型性能\n",
    "        model.eval()  # 设置模型为评估模式\n",
    "        with torch.no_grad():  # 不计算梯度，节省内存和计算资源\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            val_accuracy = (predicted == y_val).float().mean()\n",
    "        \n",
    "        # 更新学习率\n",
    "        # scheduler.step(val_loss)  # 这里使用验证损失作为参数\n",
    "        \n",
    "        # 每隔一定epoch后，打印训练状态\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}, '\n",
    "                  f'Validation Loss: {val_loss.item():.4f}, '\n",
    "                  f'Validation Accuracy: {val_accuracy.item():.4f}')\n",
    "\n",
    "    # 返回训练好的模型\n",
    "    return model\n",
    "\n",
    "# 注意：在实际使用之前，你需要定义X_train, y_train, X_val, y_val，并确保它们是正确的形状和类型，并且已经转移到了GPU\n",
    "# X_train, y_train, X_val, y_val = ...\n",
    "\n",
    "# 训练模型\n",
    "trained_model = train_model(model, criterion, optimizer, None, X_train, y_train, X_val, y_val, 20000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_val, y_val)  # 验证集\n",
    "test_dataset = TensorDataset(X_test, y_test)  # 测试集\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)  # 创建验证集的加载器\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # 测试集的加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "        super(Net, self).__init__()\n",
    "        # 使用 trial.suggest_int 来调整网络层的数量\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 10)\n",
    "        layers = []\n",
    "\n",
    "        in_features = 81 # 修改输入特征数量为 9x9 = 81\n",
    "        for i in range(n_layers):\n",
    "            out_features = trial.suggest_int('n_units_l{}'.format(i), 4, 128)\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, 10)) # 假设有 10 个类别\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 为这次 trial 创建网络\n",
    "    model = Net(trial).cuda()\n",
    "\n",
    "    # 提议超参数\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 训练网络\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 验证网络\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / len(valid_loader.dataset)\n",
    "\n",
    "    # Optuna 期望目标函数返回一个可以最小化或最大化的值\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-26 13:21:42,892] A new study created in memory with name: no-name-3464095b-92a4-4924-a931-93c6b678e86e\n",
      "[I 2024-03-26 13:21:47,984] Trial 0 finished with value: 0.1813608049832295 and parameters: {'n_layers': 3, 'n_units_l0': 71, 'n_units_l1': 118, 'n_units_l2': 72, 'optimizer': 'SGD', 'lr': 0.035010366300681724}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:21:55,788] Trial 1 finished with value: 0.12038811691423096 and parameters: {'n_layers': 5, 'n_units_l0': 63, 'n_units_l1': 104, 'n_units_l2': 41, 'n_units_l3': 128, 'n_units_l4': 47, 'optimizer': 'Adam', 'lr': 0.08215233286760888}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:03,612] Trial 2 finished with value: 0.1750119789171059 and parameters: {'n_layers': 5, 'n_units_l0': 4, 'n_units_l1': 65, 'n_units_l2': 112, 'n_units_l3': 48, 'n_units_l4': 99, 'optimizer': 'Adam', 'lr': 0.0002505123586834996}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:10,643] Trial 3 finished with value: 0.11990896022999521 and parameters: {'n_layers': 6, 'n_units_l0': 116, 'n_units_l1': 82, 'n_units_l2': 11, 'n_units_l3': 5, 'n_units_l4': 95, 'n_units_l5': 12, 'optimizer': 'SGD', 'lr': 0.0025977362753368404}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:21,936] Trial 4 finished with value: 0.127815045519885 and parameters: {'n_layers': 10, 'n_units_l0': 58, 'n_units_l1': 21, 'n_units_l2': 44, 'n_units_l3': 117, 'n_units_l4': 18, 'n_units_l5': 90, 'n_units_l6': 76, 'n_units_l7': 23, 'n_units_l8': 83, 'n_units_l9': 15, 'optimizer': 'Adam', 'lr': 1.3670693307641039e-05}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:27,763] Trial 5 finished with value: 0.12613799712505988 and parameters: {'n_layers': 4, 'n_units_l0': 66, 'n_units_l1': 67, 'n_units_l2': 68, 'n_units_l3': 33, 'optimizer': 'SGD', 'lr': 0.0021584577701495937}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:37,203] Trial 6 finished with value: 0.14734068040249163 and parameters: {'n_layers': 8, 'n_units_l0': 64, 'n_units_l1': 92, 'n_units_l2': 74, 'n_units_l3': 31, 'n_units_l4': 17, 'n_units_l5': 55, 'n_units_l6': 58, 'n_units_l7': 88, 'optimizer': 'Adam', 'lr': 1.0557343693711172e-05}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:41,954] Trial 7 finished with value: 0.159798754192621 and parameters: {'n_layers': 1, 'n_units_l0': 43, 'optimizer': 'Adam', 'lr': 2.7702608116853602e-05}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:49,563] Trial 8 finished with value: 0.11978917105893627 and parameters: {'n_layers': 8, 'n_units_l0': 34, 'n_units_l1': 86, 'n_units_l2': 95, 'n_units_l3': 108, 'n_units_l4': 64, 'n_units_l5': 103, 'n_units_l6': 86, 'n_units_l7': 34, 'optimizer': 'SGD', 'lr': 0.005519945183965176}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:22:58,565] Trial 9 finished with value: 0.1796837565884044 and parameters: {'n_layers': 8, 'n_units_l0': 41, 'n_units_l1': 96, 'n_units_l2': 56, 'n_units_l3': 127, 'n_units_l4': 89, 'n_units_l5': 4, 'n_units_l6': 73, 'n_units_l7': 72, 'optimizer': 'Adam', 'lr': 0.00014015038114511934}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:23:02,798] Trial 10 finished with value: 0.17309535218016292 and parameters: {'n_layers': 1, 'n_units_l0': 112, 'optimizer': 'RMSprop', 'lr': 0.033066818277578985}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:23:08,117] Trial 11 finished with value: 0.18004312410158121 and parameters: {'n_layers': 3, 'n_units_l0': 94, 'n_units_l1': 124, 'n_units_l2': 44, 'optimizer': 'RMSprop', 'lr': 0.0002961733644343686}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:23:13,612] Trial 12 finished with value: 0.11367992333493052 and parameters: {'n_layers': 3, 'n_units_l0': 90, 'n_units_l1': 128, 'n_units_l2': 23, 'optimizer': 'RMSprop', 'lr': 0.016610855148676953}. Best is trial 0 with value: 0.1813608049832295.\n",
      "[I 2024-03-26 13:23:19,145] Trial 13 finished with value: 0.1826784858648778 and parameters: {'n_layers': 3, 'n_units_l0': 88, 'n_units_l1': 125, 'n_units_l2': 77, 'optimizer': 'RMSprop', 'lr': 0.0005326875148807485}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:23:23,875] Trial 14 finished with value: 0.15967896502156206 and parameters: {'n_layers': 2, 'n_units_l0': 91, 'n_units_l1': 111, 'optimizer': 'SGD', 'lr': 0.0007807839239619904}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:23:29,641] Trial 15 finished with value: 0.1770483948251078 and parameters: {'n_layers': 3, 'n_units_l0': 83, 'n_units_l1': 44, 'n_units_l2': 84, 'optimizer': 'RMSprop', 'lr': 0.009477821209326645}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:23:37,586] Trial 16 finished with value: 0.1060134163871586 and parameters: {'n_layers': 6, 'n_units_l0': 105, 'n_units_l1': 115, 'n_units_l2': 123, 'n_units_l3': 84, 'n_units_l4': 124, 'n_units_l5': 127, 'optimizer': 'RMSprop', 'lr': 0.02668373026614873}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:23:43,825] Trial 17 finished with value: 0.1769286056540489 and parameters: {'n_layers': 4, 'n_units_l0': 125, 'n_units_l1': 7, 'n_units_l2': 97, 'n_units_l3': 84, 'optimizer': 'SGD', 'lr': 0.07969477763039172}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:23:48,594] Trial 18 finished with value: 0.17034020124580737 and parameters: {'n_layers': 2, 'n_units_l0': 79, 'n_units_l1': 71, 'optimizer': 'SGD', 'lr': 0.003587509453292676}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:23:54,900] Trial 19 finished with value: 0.18052228078581697 and parameters: {'n_layers': 4, 'n_units_l0': 13, 'n_units_l1': 112, 'n_units_l2': 86, 'n_units_l3': 71, 'optimizer': 'RMSprop', 'lr': 0.006314838881831054}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:23:59,298] Trial 20 finished with value: 0.1492573071394346 and parameters: {'n_layers': 2, 'n_units_l0': 77, 'n_units_l1': 41, 'optimizer': 'SGD', 'lr': 0.0012199849329956047}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:24:05,908] Trial 21 finished with value: 0.17944417824628653 and parameters: {'n_layers': 4, 'n_units_l0': 17, 'n_units_l1': 116, 'n_units_l2': 85, 'n_units_l3': 69, 'optimizer': 'RMSprop', 'lr': 0.008533938030315288}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:24:12,379] Trial 22 finished with value: 0.1641111643507427 and parameters: {'n_layers': 4, 'n_units_l0': 50, 'n_units_l1': 103, 'n_units_l2': 59, 'n_units_l3': 96, 'optimizer': 'RMSprop', 'lr': 0.011467161664548284}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:24:17,979] Trial 23 finished with value: 0.18052228078581697 and parameters: {'n_layers': 3, 'n_units_l0': 22, 'n_units_l1': 128, 'n_units_l2': 77, 'optimizer': 'RMSprop', 'lr': 0.005251905622247473}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:24:24,579] Trial 24 finished with value: 0.12793483469094394 and parameters: {'n_layers': 5, 'n_units_l0': 101, 'n_units_l1': 109, 'n_units_l2': 101, 'n_units_l3': 57, 'n_units_l4': 39, 'optimizer': 'RMSprop', 'lr': 0.02753757871481257}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:24:32,729] Trial 25 finished with value: 0.17788691902252035 and parameters: {'n_layers': 7, 'n_units_l0': 70, 'n_units_l1': 119, 'n_units_l2': 88, 'n_units_l3': 9, 'n_units_l4': 127, 'n_units_l5': 49, 'n_units_l6': 7, 'optimizer': 'RMSprop', 'lr': 0.0011798539297088256}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[I 2024-03-26 13:24:37,818] Trial 26 finished with value: 0.1638715860086248 and parameters: {'n_layers': 2, 'n_units_l0': 54, 'n_units_l1': 99, 'optimizer': 'RMSprop', 'lr': 0.039312403459675516}. Best is trial 13 with value: 0.1826784858648778.\n",
      "[W 2024-03-26 13:24:38,005] Trial 27 failed with parameters: {'n_layers': 1, 'n_units_l0': 29, 'optimizer': 'SGD', 'lr': 0.01384442988381655} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\14390\\AppData\\Local\\Temp\\ipykernel_29720\\762546424.py\", line 20, in objective\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-26 13:24:38,009] Trial 27 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 创建一个 study 对象并运行优化\u001b[39;00m\n\u001b[0;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 最佳参数\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of finished trials:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     19\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m---> 20\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 验证网络\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 创建一个 study 对象并运行优化\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# 最佳参数\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0.1889075227599425\n",
      "Params: \n",
      "    n_layers: 4\n",
      "    n_units_l0: 46\n",
      "    n_units_l1: 88\n",
      "    n_units_l2: 26\n",
      "    n_units_l3: 73\n",
      "    optimizer: Adam\n",
      "    lr: 0.000951897561635773\n"
     ]
    }
   ],
   "source": [
    "print('Value: {}'.format(trial.value))\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将study对象保存到文件\n",
    "with open('Multiclass_Study.pkl', 'wb') as f:\n",
    "    pickle.dump(study, f)\n",
    "\n",
    "# 稍后时间点，从文件加载study对象\n",
    "with open('Multiclass_Study.pkl', 'rb') as f:\n",
    "    loaded_study = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最佳参数到文件\n",
    "joblib.dump(trial.params, 'best_params.pkl')\n",
    "\n",
    "# 需要时加载最佳参数\n",
    "best_params = joblib.load('best_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300:   0%|          | 0/392 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 392/392 [00:01<00:00, 307.58batch/s, loss=2.18]\n",
      "Epoch 2/300: 100%|██████████| 392/392 [00:01<00:00, 313.89batch/s, loss=2.15]\n",
      "Epoch 3/300: 100%|██████████| 392/392 [00:01<00:00, 312.41batch/s, loss=2.14]\n",
      "Epoch 4/300: 100%|██████████| 392/392 [00:01<00:00, 323.44batch/s, loss=2.14]\n",
      "Epoch 5/300: 100%|██████████| 392/392 [00:01<00:00, 307.02batch/s, loss=2.14]\n",
      "Epoch 6/300: 100%|██████████| 392/392 [00:01<00:00, 312.82batch/s, loss=2.14]\n",
      "Epoch 7/300: 100%|██████████| 392/392 [00:01<00:00, 318.26batch/s, loss=2.13]\n",
      "Epoch 8/300: 100%|██████████| 392/392 [00:01<00:00, 315.82batch/s, loss=2.13]\n",
      "Epoch 9/300: 100%|██████████| 392/392 [00:01<00:00, 339.84batch/s, loss=2.13]\n",
      "Epoch 10/300: 100%|██████████| 392/392 [00:01<00:00, 330.89batch/s, loss=2.12]\n",
      "Epoch 11/300: 100%|██████████| 392/392 [00:01<00:00, 346.06batch/s, loss=2.12]\n",
      "Epoch 12/300: 100%|██████████| 392/392 [00:01<00:00, 337.95batch/s, loss=2.12]\n",
      "Epoch 13/300: 100%|██████████| 392/392 [00:01<00:00, 331.75batch/s, loss=2.12]\n",
      "Epoch 14/300: 100%|██████████| 392/392 [00:01<00:00, 347.76batch/s, loss=2.12]\n",
      "Epoch 15/300: 100%|██████████| 392/392 [00:01<00:00, 364.26batch/s, loss=2.12]\n",
      "Epoch 16/300: 100%|██████████| 392/392 [00:01<00:00, 352.11batch/s, loss=2.11]\n",
      "Epoch 17/300: 100%|██████████| 392/392 [00:01<00:00, 355.62batch/s, loss=2.11]\n",
      "Epoch 18/300: 100%|██████████| 392/392 [00:01<00:00, 356.90batch/s, loss=2.11]\n",
      "Epoch 19/300: 100%|██████████| 392/392 [00:01<00:00, 349.69batch/s, loss=2.11]\n",
      "Epoch 20/300: 100%|██████████| 392/392 [00:01<00:00, 357.53batch/s, loss=2.11]\n",
      "Epoch 21/300: 100%|██████████| 392/392 [00:01<00:00, 346.72batch/s, loss=2.1] \n",
      "Epoch 22/300: 100%|██████████| 392/392 [00:01<00:00, 350.69batch/s, loss=2.1] \n",
      "Epoch 23/300: 100%|██████████| 392/392 [00:01<00:00, 346.41batch/s, loss=2.1]\n",
      "Epoch 24/300: 100%|██████████| 392/392 [00:01<00:00, 358.93batch/s, loss=2.1] \n",
      "Epoch 25/300: 100%|██████████| 392/392 [00:01<00:00, 341.81batch/s, loss=2.09]\n",
      "Epoch 26/300: 100%|██████████| 392/392 [00:01<00:00, 330.74batch/s, loss=2.09]\n",
      "Epoch 27/300: 100%|██████████| 392/392 [00:01<00:00, 340.74batch/s, loss=2.09]\n",
      "Epoch 28/300: 100%|██████████| 392/392 [00:01<00:00, 339.62batch/s, loss=2.09]\n",
      "Epoch 29/300: 100%|██████████| 392/392 [00:01<00:00, 335.62batch/s, loss=2.09]\n",
      "Epoch 30/300: 100%|██████████| 392/392 [00:01<00:00, 346.63batch/s, loss=2.08]\n",
      "Epoch 31/300: 100%|██████████| 392/392 [00:01<00:00, 357.10batch/s, loss=2.08]\n",
      "Epoch 32/300: 100%|██████████| 392/392 [00:01<00:00, 353.64batch/s, loss=2.08]\n",
      "Epoch 33/300: 100%|██████████| 392/392 [00:01<00:00, 350.17batch/s, loss=2.08]\n",
      "Epoch 34/300: 100%|██████████| 392/392 [00:01<00:00, 356.99batch/s, loss=2.07]\n",
      "Epoch 35/300: 100%|██████████| 392/392 [00:01<00:00, 356.18batch/s, loss=2.07]\n",
      "Epoch 36/300: 100%|██████████| 392/392 [00:01<00:00, 241.07batch/s, loss=2.07]\n",
      "Epoch 37/300: 100%|██████████| 392/392 [00:01<00:00, 216.75batch/s, loss=2.07]\n",
      "Epoch 38/300: 100%|██████████| 392/392 [00:01<00:00, 323.04batch/s, loss=2.06]\n",
      "Epoch 39/300: 100%|██████████| 392/392 [00:01<00:00, 340.73batch/s, loss=2.06]\n",
      "Epoch 40/300: 100%|██████████| 392/392 [00:01<00:00, 350.69batch/s, loss=2.06]\n",
      "Epoch 41/300: 100%|██████████| 392/392 [00:01<00:00, 356.86batch/s, loss=2.06]\n",
      "Epoch 42/300: 100%|██████████| 392/392 [00:01<00:00, 352.46batch/s, loss=2.05]\n",
      "Epoch 43/300: 100%|██████████| 392/392 [00:01<00:00, 340.48batch/s, loss=2.05]\n",
      "Epoch 44/300: 100%|██████████| 392/392 [00:01<00:00, 334.79batch/s, loss=2.05]\n",
      "Epoch 45/300: 100%|██████████| 392/392 [00:01<00:00, 329.28batch/s, loss=2.05]\n",
      "Epoch 46/300: 100%|██████████| 392/392 [00:01<00:00, 345.23batch/s, loss=2.05]\n",
      "Epoch 47/300: 100%|██████████| 392/392 [00:01<00:00, 332.77batch/s, loss=2.04]\n",
      "Epoch 48/300: 100%|██████████| 392/392 [00:01<00:00, 341.02batch/s, loss=2.04]\n",
      "Epoch 49/300: 100%|██████████| 392/392 [00:01<00:00, 332.07batch/s, loss=2.04]\n",
      "Epoch 50/300: 100%|██████████| 392/392 [00:01<00:00, 321.71batch/s, loss=2.04]\n",
      "Epoch 51/300: 100%|██████████| 392/392 [00:01<00:00, 313.48batch/s, loss=2.03]\n",
      "Epoch 52/300: 100%|██████████| 392/392 [00:01<00:00, 326.94batch/s, loss=2.03]\n",
      "Epoch 53/300: 100%|██████████| 392/392 [00:01<00:00, 338.08batch/s, loss=2.03]\n",
      "Epoch 54/300: 100%|██████████| 392/392 [00:01<00:00, 330.39batch/s, loss=2.03]\n",
      "Epoch 55/300: 100%|██████████| 392/392 [00:01<00:00, 328.73batch/s, loss=2.02]\n",
      "Epoch 56/300: 100%|██████████| 392/392 [00:01<00:00, 336.63batch/s, loss=2.02]\n",
      "Epoch 57/300: 100%|██████████| 392/392 [00:01<00:00, 338.62batch/s, loss=2.02]\n",
      "Epoch 58/300: 100%|██████████| 392/392 [00:01<00:00, 337.17batch/s, loss=2.02]\n",
      "Epoch 59/300: 100%|██████████| 392/392 [00:01<00:00, 324.69batch/s, loss=2.01]\n",
      "Epoch 60/300: 100%|██████████| 392/392 [00:01<00:00, 356.52batch/s, loss=2.01]\n",
      "Epoch 61/300: 100%|██████████| 392/392 [00:01<00:00, 341.84batch/s, loss=2.01]\n",
      "Epoch 62/300: 100%|██████████| 392/392 [00:01<00:00, 350.62batch/s, loss=2.01]\n",
      "Epoch 63/300: 100%|██████████| 392/392 [00:01<00:00, 334.90batch/s, loss=2.01]\n",
      "Epoch 64/300: 100%|██████████| 392/392 [00:01<00:00, 326.94batch/s, loss=2]   \n",
      "Epoch 65/300: 100%|██████████| 392/392 [00:01<00:00, 330.67batch/s, loss=2]   \n",
      "Epoch 66/300: 100%|██████████| 392/392 [00:01<00:00, 337.21batch/s, loss=2]   \n",
      "Epoch 67/300: 100%|██████████| 392/392 [00:01<00:00, 295.52batch/s, loss=2]   \n",
      "Epoch 68/300: 100%|██████████| 392/392 [00:01<00:00, 300.62batch/s, loss=2]   \n",
      "Epoch 69/300: 100%|██████████| 392/392 [00:01<00:00, 324.64batch/s, loss=2]   \n",
      "Epoch 70/300: 100%|██████████| 392/392 [00:01<00:00, 327.63batch/s, loss=1.99]\n",
      "Epoch 71/300: 100%|██████████| 392/392 [00:01<00:00, 313.73batch/s, loss=1.99]\n",
      "Epoch 72/300: 100%|██████████| 392/392 [00:01<00:00, 326.80batch/s, loss=1.99]\n",
      "Epoch 73/300: 100%|██████████| 392/392 [00:01<00:00, 329.97batch/s, loss=1.99]\n",
      "Epoch 74/300: 100%|██████████| 392/392 [00:01<00:00, 322.24batch/s, loss=1.99]\n",
      "Epoch 75/300: 100%|██████████| 392/392 [00:01<00:00, 326.81batch/s, loss=1.98]\n",
      "Epoch 76/300: 100%|██████████| 392/392 [00:01<00:00, 327.35batch/s, loss=1.98]\n",
      "Epoch 77/300: 100%|██████████| 392/392 [00:01<00:00, 268.85batch/s, loss=1.98]\n",
      "Epoch 78/300: 100%|██████████| 392/392 [00:01<00:00, 279.84batch/s, loss=1.98]\n",
      "Epoch 79/300: 100%|██████████| 392/392 [00:01<00:00, 284.13batch/s, loss=1.98]\n",
      "Epoch 80/300: 100%|██████████| 392/392 [00:01<00:00, 330.52batch/s, loss=1.97]\n",
      "Epoch 81/300: 100%|██████████| 392/392 [00:01<00:00, 332.35batch/s, loss=1.97]\n",
      "Epoch 82/300: 100%|██████████| 392/392 [00:01<00:00, 295.06batch/s, loss=1.97]\n",
      "Epoch 83/300: 100%|██████████| 392/392 [00:01<00:00, 289.15batch/s, loss=1.97]\n",
      "Epoch 84/300: 100%|██████████| 392/392 [00:01<00:00, 324.54batch/s, loss=1.97]\n",
      "Epoch 85/300: 100%|██████████| 392/392 [00:01<00:00, 336.06batch/s, loss=1.97]\n",
      "Epoch 86/300: 100%|██████████| 392/392 [00:01<00:00, 340.70batch/s, loss=1.96]\n",
      "Epoch 87/300: 100%|██████████| 392/392 [00:01<00:00, 339.42batch/s, loss=1.96]\n",
      "Epoch 88/300: 100%|██████████| 392/392 [00:01<00:00, 362.00batch/s, loss=1.96]\n",
      "Epoch 89/300: 100%|██████████| 392/392 [00:01<00:00, 355.89batch/s, loss=1.96]\n",
      "Epoch 90/300: 100%|██████████| 392/392 [00:01<00:00, 354.11batch/s, loss=1.96]\n",
      "Epoch 91/300: 100%|██████████| 392/392 [00:01<00:00, 326.49batch/s, loss=1.96]\n",
      "Epoch 92/300: 100%|██████████| 392/392 [00:01<00:00, 354.49batch/s, loss=1.96]\n",
      "Epoch 93/300: 100%|██████████| 392/392 [00:01<00:00, 338.17batch/s, loss=1.95]\n",
      "Epoch 94/300: 100%|██████████| 392/392 [00:01<00:00, 337.40batch/s, loss=1.95]\n",
      "Epoch 95/300: 100%|██████████| 392/392 [00:01<00:00, 357.35batch/s, loss=1.95]\n",
      "Epoch 96/300: 100%|██████████| 392/392 [00:01<00:00, 361.93batch/s, loss=1.95]\n",
      "Epoch 97/300: 100%|██████████| 392/392 [00:01<00:00, 363.09batch/s, loss=1.94]\n",
      "Epoch 98/300: 100%|██████████| 392/392 [00:01<00:00, 357.00batch/s, loss=1.94]\n",
      "Epoch 99/300: 100%|██████████| 392/392 [00:01<00:00, 370.40batch/s, loss=1.94]\n",
      "Epoch 100/300: 100%|██████████| 392/392 [00:01<00:00, 368.26batch/s, loss=1.94]\n",
      "Epoch 101/300: 100%|██████████| 392/392 [00:01<00:00, 371.37batch/s, loss=1.94]\n",
      "Epoch 102/300: 100%|██████████| 392/392 [00:01<00:00, 363.37batch/s, loss=1.94]\n",
      "Epoch 103/300: 100%|██████████| 392/392 [00:01<00:00, 369.62batch/s, loss=1.94]\n",
      "Epoch 104/300: 100%|██████████| 392/392 [00:01<00:00, 371.58batch/s, loss=1.94]\n",
      "Epoch 105/300: 100%|██████████| 392/392 [00:01<00:00, 364.32batch/s, loss=1.93]\n",
      "Epoch 106/300: 100%|██████████| 392/392 [00:01<00:00, 340.73batch/s, loss=1.93]\n",
      "Epoch 107/300: 100%|██████████| 392/392 [00:01<00:00, 354.03batch/s, loss=1.93]\n",
      "Epoch 108/300: 100%|██████████| 392/392 [00:01<00:00, 351.37batch/s, loss=1.93]\n",
      "Epoch 109/300: 100%|██████████| 392/392 [00:01<00:00, 351.50batch/s, loss=1.93]\n",
      "Epoch 110/300: 100%|██████████| 392/392 [00:01<00:00, 359.96batch/s, loss=1.93]\n",
      "Epoch 111/300: 100%|██████████| 392/392 [00:01<00:00, 369.41batch/s, loss=1.93]\n",
      "Epoch 112/300: 100%|██████████| 392/392 [00:01<00:00, 368.97batch/s, loss=1.92]\n",
      "Epoch 113/300: 100%|██████████| 392/392 [00:01<00:00, 377.01batch/s, loss=1.92]\n",
      "Epoch 114/300: 100%|██████████| 392/392 [00:01<00:00, 372.11batch/s, loss=1.92]\n",
      "Epoch 115/300: 100%|██████████| 392/392 [00:01<00:00, 362.82batch/s, loss=1.92]\n",
      "Epoch 116/300: 100%|██████████| 392/392 [00:01<00:00, 373.87batch/s, loss=1.92]\n",
      "Epoch 117/300: 100%|██████████| 392/392 [00:01<00:00, 368.53batch/s, loss=1.92]\n",
      "Epoch 118/300: 100%|██████████| 392/392 [00:01<00:00, 346.76batch/s, loss=1.92]\n",
      "Epoch 119/300: 100%|██████████| 392/392 [00:01<00:00, 365.80batch/s, loss=1.91]\n",
      "Epoch 120/300: 100%|██████████| 392/392 [00:01<00:00, 347.72batch/s, loss=1.91]\n",
      "Epoch 121/300: 100%|██████████| 392/392 [00:01<00:00, 341.11batch/s, loss=1.91]\n",
      "Epoch 122/300: 100%|██████████| 392/392 [00:01<00:00, 329.93batch/s, loss=1.91]\n",
      "Epoch 123/300: 100%|██████████| 392/392 [00:01<00:00, 345.81batch/s, loss=1.91]\n",
      "Epoch 124/300: 100%|██████████| 392/392 [00:01<00:00, 342.13batch/s, loss=1.91]\n",
      "Epoch 125/300: 100%|██████████| 392/392 [00:01<00:00, 345.75batch/s, loss=1.91]\n",
      "Epoch 126/300: 100%|██████████| 392/392 [00:01<00:00, 360.23batch/s, loss=1.9] \n",
      "Epoch 127/300: 100%|██████████| 392/392 [00:01<00:00, 349.59batch/s, loss=1.9] \n",
      "Epoch 128/300: 100%|██████████| 392/392 [00:01<00:00, 361.11batch/s, loss=1.9] \n",
      "Epoch 129/300: 100%|██████████| 392/392 [00:01<00:00, 354.10batch/s, loss=1.9] \n",
      "Epoch 130/300: 100%|██████████| 392/392 [00:01<00:00, 361.30batch/s, loss=1.9] \n",
      "Epoch 131/300: 100%|██████████| 392/392 [00:01<00:00, 361.98batch/s, loss=1.9] \n",
      "Epoch 132/300: 100%|██████████| 392/392 [00:01<00:00, 367.57batch/s, loss=1.9] \n",
      "Epoch 133/300: 100%|██████████| 392/392 [00:01<00:00, 367.90batch/s, loss=1.9] \n",
      "Epoch 134/300: 100%|██████████| 392/392 [00:01<00:00, 359.14batch/s, loss=1.89]\n",
      "Epoch 135/300: 100%|██████████| 392/392 [00:01<00:00, 360.63batch/s, loss=1.89]\n",
      "Epoch 136/300: 100%|██████████| 392/392 [00:01<00:00, 358.16batch/s, loss=1.89]\n",
      "Epoch 137/300: 100%|██████████| 392/392 [00:01<00:00, 356.22batch/s, loss=1.89]\n",
      "Epoch 138/300: 100%|██████████| 392/392 [00:01<00:00, 346.20batch/s, loss=1.89]\n",
      "Epoch 139/300: 100%|██████████| 392/392 [00:01<00:00, 365.36batch/s, loss=1.89]\n",
      "Epoch 140/300: 100%|██████████| 392/392 [00:01<00:00, 358.82batch/s, loss=1.89]\n",
      "Epoch 141/300: 100%|██████████| 392/392 [00:01<00:00, 358.93batch/s, loss=1.88]\n",
      "Epoch 142/300: 100%|██████████| 392/392 [00:01<00:00, 340.45batch/s, loss=1.89]\n",
      "Epoch 143/300: 100%|██████████| 392/392 [00:01<00:00, 352.29batch/s, loss=1.88]\n",
      "Epoch 144/300: 100%|██████████| 392/392 [00:01<00:00, 355.67batch/s, loss=1.88]\n",
      "Epoch 145/300: 100%|██████████| 392/392 [00:01<00:00, 355.39batch/s, loss=1.88]\n",
      "Epoch 146/300: 100%|██████████| 392/392 [00:01<00:00, 356.16batch/s, loss=1.88]\n",
      "Epoch 147/300: 100%|██████████| 392/392 [00:01<00:00, 356.79batch/s, loss=1.88]\n",
      "Epoch 148/300: 100%|██████████| 392/392 [00:01<00:00, 355.86batch/s, loss=1.88]\n",
      "Epoch 149/300: 100%|██████████| 392/392 [00:01<00:00, 350.40batch/s, loss=1.88]\n",
      "Epoch 150/300: 100%|██████████| 392/392 [00:01<00:00, 364.51batch/s, loss=1.87]\n",
      "Epoch 151/300: 100%|██████████| 392/392 [00:01<00:00, 363.83batch/s, loss=1.88]\n",
      "Epoch 152/300: 100%|██████████| 392/392 [00:01<00:00, 352.88batch/s, loss=1.87]\n",
      "Epoch 153/300: 100%|██████████| 392/392 [00:01<00:00, 356.92batch/s, loss=1.87]\n",
      "Epoch 154/300: 100%|██████████| 392/392 [00:01<00:00, 370.85batch/s, loss=1.87]\n",
      "Epoch 155/300: 100%|██████████| 392/392 [00:01<00:00, 373.45batch/s, loss=1.87]\n",
      "Epoch 156/300: 100%|██████████| 392/392 [00:01<00:00, 363.53batch/s, loss=1.87]\n",
      "Epoch 157/300: 100%|██████████| 392/392 [00:01<00:00, 369.24batch/s, loss=1.87]\n",
      "Epoch 158/300: 100%|██████████| 392/392 [00:01<00:00, 369.69batch/s, loss=1.87]\n",
      "Epoch 159/300: 100%|██████████| 392/392 [00:01<00:00, 368.49batch/s, loss=1.87]\n",
      "Epoch 160/300: 100%|██████████| 392/392 [00:01<00:00, 368.77batch/s, loss=1.87]\n",
      "Epoch 161/300: 100%|██████████| 392/392 [00:01<00:00, 373.51batch/s, loss=1.86]\n",
      "Epoch 162/300: 100%|██████████| 392/392 [00:01<00:00, 361.73batch/s, loss=1.87]\n",
      "Epoch 163/300: 100%|██████████| 392/392 [00:01<00:00, 363.79batch/s, loss=1.86]\n",
      "Epoch 164/300: 100%|██████████| 392/392 [00:01<00:00, 367.38batch/s, loss=1.86]\n",
      "Epoch 165/300: 100%|██████████| 392/392 [00:01<00:00, 368.54batch/s, loss=1.86]\n",
      "Epoch 166/300: 100%|██████████| 392/392 [00:01<00:00, 362.29batch/s, loss=1.86]\n",
      "Epoch 167/300: 100%|██████████| 392/392 [00:01<00:00, 362.42batch/s, loss=1.86]\n",
      "Epoch 168/300: 100%|██████████| 392/392 [00:01<00:00, 364.23batch/s, loss=1.86]\n",
      "Epoch 169/300: 100%|██████████| 392/392 [00:01<00:00, 351.55batch/s, loss=1.86]\n",
      "Epoch 170/300: 100%|██████████| 392/392 [00:01<00:00, 349.33batch/s, loss=1.86]\n",
      "Epoch 171/300: 100%|██████████| 392/392 [00:01<00:00, 348.33batch/s, loss=1.86]\n",
      "Epoch 172/300: 100%|██████████| 392/392 [00:01<00:00, 362.58batch/s, loss=1.85]\n",
      "Epoch 173/300: 100%|██████████| 392/392 [00:01<00:00, 366.99batch/s, loss=1.85]\n",
      "Epoch 174/300: 100%|██████████| 392/392 [00:01<00:00, 371.68batch/s, loss=1.85]\n",
      "Epoch 175/300: 100%|██████████| 392/392 [00:01<00:00, 362.85batch/s, loss=1.85]\n",
      "Epoch 176/300: 100%|██████████| 392/392 [00:01<00:00, 373.74batch/s, loss=1.85]\n",
      "Epoch 177/300: 100%|██████████| 392/392 [00:01<00:00, 373.99batch/s, loss=1.85]\n",
      "Epoch 178/300: 100%|██████████| 392/392 [00:01<00:00, 376.96batch/s, loss=1.85]\n",
      "Epoch 179/300: 100%|██████████| 392/392 [00:01<00:00, 368.45batch/s, loss=1.85]\n",
      "Epoch 180/300: 100%|██████████| 392/392 [00:01<00:00, 385.41batch/s, loss=1.85]\n",
      "Epoch 181/300: 100%|██████████| 392/392 [00:01<00:00, 384.18batch/s, loss=1.85]\n",
      "Epoch 182/300: 100%|██████████| 392/392 [00:01<00:00, 374.76batch/s, loss=1.85]\n",
      "Epoch 183/300: 100%|██████████| 392/392 [00:01<00:00, 357.53batch/s, loss=1.84]\n",
      "Epoch 184/300: 100%|██████████| 392/392 [00:01<00:00, 370.37batch/s, loss=1.85]\n",
      "Epoch 185/300: 100%|██████████| 392/392 [00:01<00:00, 364.00batch/s, loss=1.84]\n",
      "Epoch 186/300: 100%|██████████| 392/392 [00:01<00:00, 367.43batch/s, loss=1.84]\n",
      "Epoch 187/300: 100%|██████████| 392/392 [00:01<00:00, 365.52batch/s, loss=1.84]\n",
      "Epoch 188/300: 100%|██████████| 392/392 [00:01<00:00, 363.81batch/s, loss=1.84]\n",
      "Epoch 189/300: 100%|██████████| 392/392 [00:01<00:00, 360.02batch/s, loss=1.84]\n",
      "Epoch 190/300: 100%|██████████| 392/392 [00:01<00:00, 364.41batch/s, loss=1.84]\n",
      "Epoch 191/300: 100%|██████████| 392/392 [00:01<00:00, 361.78batch/s, loss=1.84]\n",
      "Epoch 192/300: 100%|██████████| 392/392 [00:01<00:00, 355.76batch/s, loss=1.84]\n",
      "Epoch 193/300: 100%|██████████| 392/392 [00:01<00:00, 356.89batch/s, loss=1.84]\n",
      "Epoch 194/300: 100%|██████████| 392/392 [00:01<00:00, 355.68batch/s, loss=1.84]\n",
      "Epoch 195/300: 100%|██████████| 392/392 [00:01<00:00, 357.12batch/s, loss=1.84]\n",
      "Epoch 196/300: 100%|██████████| 392/392 [00:01<00:00, 361.06batch/s, loss=1.83]\n",
      "Epoch 197/300: 100%|██████████| 392/392 [00:01<00:00, 357.68batch/s, loss=1.83]\n",
      "Epoch 198/300: 100%|██████████| 392/392 [00:01<00:00, 353.01batch/s, loss=1.83]\n",
      "Epoch 199/300: 100%|██████████| 392/392 [00:01<00:00, 351.20batch/s, loss=1.83]\n",
      "Epoch 200/300: 100%|██████████| 392/392 [00:01<00:00, 353.77batch/s, loss=1.83]\n",
      "Epoch 201/300: 100%|██████████| 392/392 [00:01<00:00, 355.15batch/s, loss=1.83]\n",
      "Epoch 202/300: 100%|██████████| 392/392 [00:01<00:00, 353.72batch/s, loss=1.83]\n",
      "Epoch 203/300: 100%|██████████| 392/392 [00:01<00:00, 352.67batch/s, loss=1.83]\n",
      "Epoch 204/300: 100%|██████████| 392/392 [00:01<00:00, 355.27batch/s, loss=1.83]\n",
      "Epoch 205/300: 100%|██████████| 392/392 [00:01<00:00, 342.13batch/s, loss=1.83]\n",
      "Epoch 206/300: 100%|██████████| 392/392 [00:01<00:00, 349.58batch/s, loss=1.83]\n",
      "Epoch 207/300: 100%|██████████| 392/392 [00:01<00:00, 353.41batch/s, loss=1.83]\n",
      "Epoch 208/300: 100%|██████████| 392/392 [00:01<00:00, 361.45batch/s, loss=1.82]\n",
      "Epoch 209/300: 100%|██████████| 392/392 [00:01<00:00, 374.04batch/s, loss=1.82]\n",
      "Epoch 210/300: 100%|██████████| 392/392 [00:01<00:00, 383.62batch/s, loss=1.82]\n",
      "Epoch 211/300: 100%|██████████| 392/392 [00:01<00:00, 375.86batch/s, loss=1.82]\n",
      "Epoch 212/300: 100%|██████████| 392/392 [00:01<00:00, 367.72batch/s, loss=1.82]\n",
      "Epoch 213/300: 100%|██████████| 392/392 [00:01<00:00, 381.75batch/s, loss=1.82]\n",
      "Epoch 214/300: 100%|██████████| 392/392 [00:01<00:00, 365.99batch/s, loss=1.82]\n",
      "Epoch 215/300: 100%|██████████| 392/392 [00:01<00:00, 359.81batch/s, loss=1.82]\n",
      "Epoch 216/300: 100%|██████████| 392/392 [00:01<00:00, 365.04batch/s, loss=1.82]\n",
      "Epoch 217/300: 100%|██████████| 392/392 [00:01<00:00, 354.21batch/s, loss=1.82]\n",
      "Epoch 218/300: 100%|██████████| 392/392 [00:01<00:00, 359.82batch/s, loss=1.82]\n",
      "Epoch 219/300: 100%|██████████| 392/392 [00:01<00:00, 363.19batch/s, loss=1.82]\n",
      "Epoch 220/300: 100%|██████████| 392/392 [00:01<00:00, 377.10batch/s, loss=1.82]\n",
      "Epoch 221/300: 100%|██████████| 392/392 [00:01<00:00, 360.71batch/s, loss=1.82]\n",
      "Epoch 222/300: 100%|██████████| 392/392 [00:01<00:00, 364.24batch/s, loss=1.81]\n",
      "Epoch 223/300: 100%|██████████| 392/392 [00:01<00:00, 365.26batch/s, loss=1.82]\n",
      "Epoch 224/300: 100%|██████████| 392/392 [00:01<00:00, 369.53batch/s, loss=1.82]\n",
      "Epoch 225/300: 100%|██████████| 392/392 [00:01<00:00, 371.02batch/s, loss=1.82]\n",
      "Epoch 226/300: 100%|██████████| 392/392 [00:01<00:00, 371.38batch/s, loss=1.81]\n",
      "Epoch 227/300: 100%|██████████| 392/392 [00:01<00:00, 358.99batch/s, loss=1.81]\n",
      "Epoch 228/300: 100%|██████████| 392/392 [00:01<00:00, 365.22batch/s, loss=1.81]\n",
      "Epoch 229/300: 100%|██████████| 392/392 [00:01<00:00, 365.40batch/s, loss=1.81]\n",
      "Epoch 230/300: 100%|██████████| 392/392 [00:01<00:00, 368.78batch/s, loss=1.81]\n",
      "Epoch 231/300: 100%|██████████| 392/392 [00:01<00:00, 354.42batch/s, loss=1.81]\n",
      "Epoch 232/300: 100%|██████████| 392/392 [00:01<00:00, 368.75batch/s, loss=1.81]\n",
      "Epoch 233/300: 100%|██████████| 392/392 [00:01<00:00, 361.80batch/s, loss=1.81]\n",
      "Epoch 234/300: 100%|██████████| 392/392 [00:01<00:00, 364.15batch/s, loss=1.81]\n",
      "Epoch 235/300: 100%|██████████| 392/392 [00:01<00:00, 356.63batch/s, loss=1.81]\n",
      "Epoch 236/300: 100%|██████████| 392/392 [00:01<00:00, 345.61batch/s, loss=1.81]\n",
      "Epoch 237/300: 100%|██████████| 392/392 [00:01<00:00, 352.66batch/s, loss=1.81]\n",
      "Epoch 238/300: 100%|██████████| 392/392 [00:01<00:00, 345.41batch/s, loss=1.8] \n",
      "Epoch 239/300: 100%|██████████| 392/392 [00:01<00:00, 356.38batch/s, loss=1.81]\n",
      "Epoch 240/300: 100%|██████████| 392/392 [00:01<00:00, 369.40batch/s, loss=1.8] \n",
      "Epoch 241/300: 100%|██████████| 392/392 [00:01<00:00, 377.61batch/s, loss=1.81]\n",
      "Epoch 242/300: 100%|██████████| 392/392 [00:01<00:00, 369.88batch/s, loss=1.81]\n",
      "Epoch 243/300: 100%|██████████| 392/392 [00:01<00:00, 369.57batch/s, loss=1.8] \n",
      "Epoch 244/300: 100%|██████████| 392/392 [00:01<00:00, 381.15batch/s, loss=1.8] \n",
      "Epoch 245/300: 100%|██████████| 392/392 [00:01<00:00, 360.51batch/s, loss=1.8] \n",
      "Epoch 246/300: 100%|██████████| 392/392 [00:01<00:00, 380.40batch/s, loss=1.8] \n",
      "Epoch 247/300: 100%|██████████| 392/392 [00:01<00:00, 363.90batch/s, loss=1.8] \n",
      "Epoch 248/300: 100%|██████████| 392/392 [00:01<00:00, 375.63batch/s, loss=1.8] \n",
      "Epoch 249/300: 100%|██████████| 392/392 [00:01<00:00, 379.12batch/s, loss=1.8] \n",
      "Epoch 250/300: 100%|██████████| 392/392 [00:01<00:00, 381.35batch/s, loss=1.8] \n",
      "Epoch 251/300: 100%|██████████| 392/392 [00:01<00:00, 360.97batch/s, loss=1.8] \n",
      "Epoch 252/300: 100%|██████████| 392/392 [00:01<00:00, 364.62batch/s, loss=1.8] \n",
      "Epoch 253/300: 100%|██████████| 392/392 [00:01<00:00, 367.35batch/s, loss=1.8] \n",
      "Epoch 254/300: 100%|██████████| 392/392 [00:01<00:00, 369.99batch/s, loss=1.8] \n",
      "Epoch 255/300: 100%|██████████| 392/392 [00:01<00:00, 360.71batch/s, loss=1.8] \n",
      "Epoch 256/300: 100%|██████████| 392/392 [00:01<00:00, 378.99batch/s, loss=1.8] \n",
      "Epoch 257/300: 100%|██████████| 392/392 [00:01<00:00, 365.88batch/s, loss=1.8] \n",
      "Epoch 258/300: 100%|██████████| 392/392 [00:01<00:00, 373.69batch/s, loss=1.8] \n",
      "Epoch 259/300: 100%|██████████| 392/392 [00:01<00:00, 359.78batch/s, loss=1.8] \n",
      "Epoch 260/300: 100%|██████████| 392/392 [00:01<00:00, 361.81batch/s, loss=1.79]\n",
      "Epoch 261/300: 100%|██████████| 392/392 [00:01<00:00, 364.75batch/s, loss=1.79]\n",
      "Epoch 262/300: 100%|██████████| 392/392 [00:01<00:00, 369.10batch/s, loss=1.79]\n",
      "Epoch 263/300: 100%|██████████| 392/392 [00:01<00:00, 370.51batch/s, loss=1.79]\n",
      "Epoch 264/300: 100%|██████████| 392/392 [00:01<00:00, 359.47batch/s, loss=1.79]\n",
      "Epoch 265/300: 100%|██████████| 392/392 [00:01<00:00, 365.89batch/s, loss=1.79]\n",
      "Epoch 266/300: 100%|██████████| 392/392 [00:01<00:00, 381.06batch/s, loss=1.79]\n",
      "Epoch 267/300: 100%|██████████| 392/392 [00:01<00:00, 368.66batch/s, loss=1.79]\n",
      "Epoch 268/300: 100%|██████████| 392/392 [00:01<00:00, 365.66batch/s, loss=1.79]\n",
      "Epoch 269/300: 100%|██████████| 392/392 [00:01<00:00, 368.97batch/s, loss=1.79]\n",
      "Epoch 270/300: 100%|██████████| 392/392 [00:01<00:00, 364.75batch/s, loss=1.79]\n",
      "Epoch 271/300: 100%|██████████| 392/392 [00:01<00:00, 372.27batch/s, loss=1.79]\n",
      "Epoch 272/300: 100%|██████████| 392/392 [00:01<00:00, 358.47batch/s, loss=1.79]\n",
      "Epoch 273/300: 100%|██████████| 392/392 [00:01<00:00, 370.34batch/s, loss=1.79]\n",
      "Epoch 274/300: 100%|██████████| 392/392 [00:01<00:00, 370.52batch/s, loss=1.79]\n",
      "Epoch 275/300: 100%|██████████| 392/392 [00:01<00:00, 366.94batch/s, loss=1.79]\n",
      "Epoch 276/300: 100%|██████████| 392/392 [00:01<00:00, 377.94batch/s, loss=1.79]\n",
      "Epoch 277/300: 100%|██████████| 392/392 [00:01<00:00, 373.51batch/s, loss=1.79]\n",
      "Epoch 278/300: 100%|██████████| 392/392 [00:01<00:00, 372.70batch/s, loss=1.79]\n",
      "Epoch 279/300: 100%|██████████| 392/392 [00:01<00:00, 356.86batch/s, loss=1.79]\n",
      "Epoch 280/300: 100%|██████████| 392/392 [00:01<00:00, 375.00batch/s, loss=1.79]\n",
      "Epoch 281/300: 100%|██████████| 392/392 [00:01<00:00, 374.90batch/s, loss=1.78]\n",
      "Epoch 282/300: 100%|██████████| 392/392 [00:01<00:00, 370.89batch/s, loss=1.78]\n",
      "Epoch 283/300: 100%|██████████| 392/392 [00:01<00:00, 367.19batch/s, loss=1.79]\n",
      "Epoch 284/300: 100%|██████████| 392/392 [00:01<00:00, 374.54batch/s, loss=1.78]\n",
      "Epoch 285/300: 100%|██████████| 392/392 [00:01<00:00, 370.12batch/s, loss=1.78]\n",
      "Epoch 286/300: 100%|██████████| 392/392 [00:01<00:00, 367.77batch/s, loss=1.78]\n",
      "Epoch 287/300: 100%|██████████| 392/392 [00:01<00:00, 363.92batch/s, loss=1.78]\n",
      "Epoch 288/300: 100%|██████████| 392/392 [00:01<00:00, 371.21batch/s, loss=1.78]\n",
      "Epoch 289/300: 100%|██████████| 392/392 [00:01<00:00, 365.41batch/s, loss=1.78]\n",
      "Epoch 290/300: 100%|██████████| 392/392 [00:01<00:00, 357.35batch/s, loss=1.78]\n",
      "Epoch 291/300: 100%|██████████| 392/392 [00:01<00:00, 366.41batch/s, loss=1.78]\n",
      "Epoch 292/300: 100%|██████████| 392/392 [00:01<00:00, 357.73batch/s, loss=1.78]\n",
      "Epoch 293/300: 100%|██████████| 392/392 [00:01<00:00, 363.39batch/s, loss=1.78]\n",
      "Epoch 294/300: 100%|██████████| 392/392 [00:01<00:00, 360.86batch/s, loss=1.78]\n",
      "Epoch 295/300: 100%|██████████| 392/392 [00:01<00:00, 355.80batch/s, loss=1.78]\n",
      "Epoch 296/300: 100%|██████████| 392/392 [00:01<00:00, 358.76batch/s, loss=1.78]\n",
      "Epoch 297/300: 100%|██████████| 392/392 [00:01<00:00, 363.47batch/s, loss=1.78]\n",
      "Epoch 298/300: 100%|██████████| 392/392 [00:01<00:00, 355.09batch/s, loss=1.78]\n",
      "Epoch 299/300: 100%|██████████| 392/392 [00:01<00:00, 359.94batch/s, loss=1.77]\n",
      "Epoch 300/300: 100%|██████████| 392/392 [00:01<00:00, 361.70batch/s, loss=1.78]\n"
     ]
    }
   ],
   "source": [
    "# 创建最终模型使用最佳超参数\n",
    "best_trial = study.best_trial\n",
    "final_model = Net(trial).cuda()\n",
    "NUM_EPOCHS = 300\n",
    "# 选择最佳试验中推荐的优化器并设置学习率\n",
    "optimizer_name = best_params['optimizer']\n",
    "lr = best_params['lr']\n",
    "optimizer = getattr(torch.optim, optimizer_name)(final_model.parameters(), lr=lr)\n",
    "\n",
    "# 如果需要，您可以重新定义损失函数和数据加载器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# train_loader = DataLoader(...)  # 假设您已经有了完整的训练数据加载器\n",
    "\n",
    "# 训练最终模型\n",
    "for epoch in range(NUM_EPOCHS):  # 设置您想要的训练周期数\n",
    "    final_model.train()\n",
    "    train_loss = 0\n",
    "    # 使用tqdm封装您的数据加载器\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', unit='batch')\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_tqdm):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = final_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 设置进度条的后缀显示当前平均损失\n",
    "        train_loader_tqdm.set_postfix(loss=train_loss/(batch_idx+1))\n",
    "\n",
    "\n",
    "    # 每个周期结束后可以验证模型性能，并且可以选择保存性能最好的模型\n",
    "\n",
    "# 可以选择保存模型\n",
    "torch.save(final_model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17081935793004313\n",
      "Recall: 0.16306448243243982\n",
      "ROC AUC: 0.6119960344894373\n",
      "R^2 Score: -0.39241241604133204\n"
     ]
    }
   ],
   "source": [
    "# 确保模型处于评估模式\n",
    "final_model.eval()\n",
    "\n",
    "# 准备数据结构来保存真实标签和预测值\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "# 不计算梯度，以加快计算速度并减少内存消耗\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:  # 假设你有一个test_loader\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = final_model(data)\n",
    "\n",
    "        # 获得预测概率用于ROC AUC等计算\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        predicted_probs.append(probs.cpu().numpy())\n",
    "\n",
    "        # 获取预测结果\n",
    "        _, preds = torch.max(output, 1)\n",
    "        true_labels.extend(target.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# 将列表转换为NumPy数组\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "predicted_probs = np.vstack(predicted_probs)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# 计算宏平均召回率\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# 计算ROC AUC（多分类需要处理）\n",
    "# 假设是二分类问题，使用标签为1的正类概率\n",
    "if predicted_probs.shape[1] == 2:\n",
    "    roc_auc = roc_auc_score(true_labels, predicted_probs[:, 1])\n",
    "else:\n",
    "    # 使用one-vs-rest approach处理多分类问题的ROC AUC\n",
    "    roc_auc = roc_auc_score(true_labels, predicted_probs, multi_class='ovr')\n",
    "\n",
    "# 计算R^2得分\n",
    "r2 = r2_score(true_labels, predicted_labels)\n",
    "\n",
    "# 打印所有指标\n",
    "print('Accuracy:', accuracy)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', roc_auc)\n",
    "print('R^2 Score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假设你的数据集和标签已经准备好，分别存储在 X 和 y 中\n",
    "# X 的维度是 (41740, 9)，y 的维度是 (41740,)\n",
    "X = np.random.rand(41740, 9)  # 这里用随机数据代替实际数据\n",
    "y = np.random.randint(0, 2, 41740)  # 这里用随机标签代替实际标签\n",
    "data = np.load('Classified_Data.npz')\n",
    "\n",
    "X = data['features']\n",
    "y = data['labels']\n",
    "# 分割数据集为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6158361284139914\n"
     ]
    }
   ],
   "source": [
    "# 创建决策树分类器实例\n",
    "# 由于scikit-learn没有直接实现C4.5，我们可以通过设置criterion='entropy'来使用信息增益比\n",
    "# 这是C4.5算法中用于选择特征的方法\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "# 训练分类器\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# 使用分类器进行预测\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
