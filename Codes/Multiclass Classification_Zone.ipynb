{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, r2_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16871, 9, 3, 3) (16871,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = np.load('DataSet/Classified_Data_zone.npz')\n",
    "\n",
    "Features = data['features']\n",
    "Labels = data['labels']\n",
    "\n",
    "print(Features.shape, Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=1)  # Max pooling to go from 2x2 to 1x1\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, feature_dim, time_steps):\n",
    "        super(CustomAttention, self).__init__()\n",
    "        self.query = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value = nn.Linear(feature_dim, feature_dim)\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        # 初始化attention_scores为负无穷大，确保未更新的元素在softmax后贡献为0\n",
    "        attention_scores = torch.full((x.size(0), self.time_steps, self.time_steps), float('-inf'), device=x.device)\n",
    "        \n",
    "        # 更新注意力分数，考虑每个9的倍数起始点\n",
    "        for start in range(9):  # 从0到8，为每个可能的序列起始点\n",
    "            for i in range(start, self.time_steps, 9):\n",
    "                for j in range(start, self.time_steps, 9):\n",
    "                    if i != j:  # 确保不是自己到自己\n",
    "                        attention_scores[:, i, j] = torch.sum(queries[:, i] * keys[:, j], dim=-1)\n",
    "\n",
    "        attention = F.softmax(attention_scores, dim=-1)\n",
    "        attended_values = torch.bmm(attention, values)\n",
    "        return attended_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifyCNN, self).__init__()\n",
    "        self.conv_block = ConvBlock()\n",
    "        self.attention = CustomAttention(feature_dim=32, time_steps=27)\n",
    "        self.fc = nn.Linear(32, 2)  # 保留为2，因为我们使用交叉熵损失函数\n",
    "\n",
    "    def forward(self, x, return_logits=False):\n",
    "        batch_size, time_steps, _, _ = x.shape\n",
    "        x = x.view(batch_size * time_steps, 1, 3, 3)\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(batch_size, time_steps, -1)\n",
    "        x = self.attention(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        logits = self.fc(x)\n",
    "        if return_logits:\n",
    "            return logits  # 在训练时返回原始 logits\n",
    "        return torch.argmax(logits, dim=1, keepdim=True)  # 在推理时返回类别索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9, 16, kernel_size=2, stride=1)\n",
    "        self.pool = nn.MaxPool2d(2, stride=1)\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 32)  # 正确调整全连接层的输入尺寸\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x, return_logits=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 1 * 1)  # 正确调整view的参数\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = torch.sigmoid(self.fc2(x))\n",
    "        if return_logits:\n",
    "            return logits  # 在训练时返回原始 logits\n",
    "        return torch.argmax(logits, dim=1, keepdim=True)  # 在推理时返回类别索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifyCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 每隔30个epoch，学习率乘以0.1                                                                                                      \n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集，通常保留20%作为测试集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    Features, Labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为 torch.tensor\n",
    "train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "\n",
    "# 创建训练 DataLoader\n",
    "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "batch_size = 64  # 设置batch_size为1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 对测试数据进行相同的处理\n",
    "test_features_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "# 创建测试 DataLoader\n",
    "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # 可以根据模型表现和计算资源进行调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.5128, Test Loss: 0.4911\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 2/100, Train Loss: 0.4903, Test Loss: 0.4884\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 3/100, Train Loss: 0.4895, Test Loss: 0.4856\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 4/100, Train Loss: 0.4868, Test Loss: 0.4881\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 5/100, Train Loss: 0.4852, Test Loss: 0.4853\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 6/100, Train Loss: 0.4851, Test Loss: 0.4861\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 7/100, Train Loss: 0.4839, Test Loss: 0.4853\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 8/100, Train Loss: 0.4832, Test Loss: 0.4859\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 9/100, Train Loss: 0.4826, Test Loss: 0.4869\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 10/100, Train Loss: 0.4825, Test Loss: 0.4857\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 11/100, Train Loss: 0.4823, Test Loss: 0.4856\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 12/100, Train Loss: 0.4811, Test Loss: 0.4889\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 13/100, Train Loss: 0.4808, Test Loss: 0.4867\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 14/100, Train Loss: 0.4809, Test Loss: 0.4857\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 15/100, Train Loss: 0.4810, Test Loss: 0.4880\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 16/100, Train Loss: 0.4805, Test Loss: 0.4862\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 17/100, Train Loss: 0.4802, Test Loss: 0.4927\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 18/100, Train Loss: 0.4803, Test Loss: 0.4879\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 19/100, Train Loss: 0.4809, Test Loss: 0.4885\n",
      "Current Learning Rate: 0.01\n",
      "Epoch 20/100, Train Loss: 0.4805, Test Loss: 0.4881\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 21/100, Train Loss: 0.4780, Test Loss: 0.4852\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 22/100, Train Loss: 0.4762, Test Loss: 0.4854\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 23/100, Train Loss: 0.4756, Test Loss: 0.4857\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 24/100, Train Loss: 0.4754, Test Loss: 0.4858\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 25/100, Train Loss: 0.4750, Test Loss: 0.4854\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 26/100, Train Loss: 0.4749, Test Loss: 0.4858\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 27/100, Train Loss: 0.4747, Test Loss: 0.4856\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 28/100, Train Loss: 0.4745, Test Loss: 0.4855\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 29/100, Train Loss: 0.4742, Test Loss: 0.4855\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 30/100, Train Loss: 0.4742, Test Loss: 0.4858\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 31/100, Train Loss: 0.4738, Test Loss: 0.4858\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 32/100, Train Loss: 0.4738, Test Loss: 0.4854\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 33/100, Train Loss: 0.4734, Test Loss: 0.4854\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 34/100, Train Loss: 0.4731, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 35/100, Train Loss: 0.4730, Test Loss: 0.4859\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 36/100, Train Loss: 0.4730, Test Loss: 0.4858\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 37/100, Train Loss: 0.4726, Test Loss: 0.4858\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 38/100, Train Loss: 0.4726, Test Loss: 0.4859\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 39/100, Train Loss: 0.4723, Test Loss: 0.4860\n",
      "Current Learning Rate: 0.001\n",
      "Epoch 40/100, Train Loss: 0.4722, Test Loss: 0.4862\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 41/100, Train Loss: 0.4714, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 42/100, Train Loss: 0.4714, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 43/100, Train Loss: 0.4713, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 44/100, Train Loss: 0.4713, Test Loss: 0.4864\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 45/100, Train Loss: 0.4713, Test Loss: 0.4864\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 46/100, Train Loss: 0.4713, Test Loss: 0.4864\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 47/100, Train Loss: 0.4712, Test Loss: 0.4864\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 48/100, Train Loss: 0.4712, Test Loss: 0.4864\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 49/100, Train Loss: 0.4712, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 50/100, Train Loss: 0.4712, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 51/100, Train Loss: 0.4712, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 52/100, Train Loss: 0.4711, Test Loss: 0.4864\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 53/100, Train Loss: 0.4711, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 54/100, Train Loss: 0.4711, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 55/100, Train Loss: 0.4711, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 56/100, Train Loss: 0.4711, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 57/100, Train Loss: 0.4711, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 58/100, Train Loss: 0.4711, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 59/100, Train Loss: 0.4711, Test Loss: 0.4863\n",
      "Current Learning Rate: 0.0001\n",
      "Epoch 60/100, Train Loss: 0.4710, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 61/100, Train Loss: 0.4710, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 62/100, Train Loss: 0.4710, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 63/100, Train Loss: 0.4710, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 64/100, Train Loss: 0.4710, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 65/100, Train Loss: 0.4709, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 66/100, Train Loss: 0.4709, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 67/100, Train Loss: 0.4709, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 68/100, Train Loss: 0.4709, Test Loss: 0.4863\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 69/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 70/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 71/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 72/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 73/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 74/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 75/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 76/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 77/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 78/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 79/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1e-05\n",
      "Epoch 80/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 81/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 82/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 83/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 84/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 85/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 86/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 87/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 88/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 89/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 90/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 91/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 92/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 93/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 94/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 95/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 96/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 97/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 98/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 99/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 100/100, Train Loss: 0.4709, Test Loss: 0.4864\n",
      "Current Learning Rate: 1.0000000000000002e-07\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# 训练函数\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, return_logits=True)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    scheduler.step()  # 更新学习率\n",
    "\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device).long()\n",
    "            output = model(data, return_logits=True)\n",
    "            loss = criterion(output, target)\n",
    "            total_test_loss += loss.item() * data.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "    print(f'Current Learning Rate: {scheduler.get_last_lr()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # 进行预测\n",
    "        output = model(data)\n",
    "        preds = output\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_test.extend(target.cpu().numpy())\n",
    "\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "y_test = np.array(y_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.79      0.82      1872\n",
      "         1.0       0.76      0.84      0.80      1503\n",
      "\n",
      "    accuracy                           0.81      3375\n",
      "   macro avg       0.81      0.81      0.81      3375\n",
      "weighted avg       0.82      0.81      0.81      3375\n",
      "\n",
      "Accuracy: 0.8118518518518518\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    batch = default_collate(batch)  # 首先使用默认的方式合并数据\n",
    "    batch = (item.squeeze(0) for item in batch)  # 去除每个item的批次维度\n",
    "    return batch\n",
    "\n",
    "# 分割数据集，通常保留20%作为测试集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    Features, Labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为 torch.tensor\n",
    "train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "\n",
    "# 对测试数据进行相同的处理\n",
    "test_features_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "# 使用自定义的 collate_fn 创建 DataLoader\n",
    "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATClassifier, self).__init__()\n",
    "        self.edge_index = self.build_adjacency_matrix().long()\n",
    "        self.conv1 = GATConv(in_channels=9, out_channels=16, heads=4, dropout=0.6)\n",
    "        self.conv2 = GATConv(in_channels=64, out_channels=16, heads=1, dropout=0.6)\n",
    "        self.fc = torch.nn.Linear(16 * 9, 2)  # 假设是9个节点\n",
    "\n",
    "    def build_adjacency_matrix(self):\n",
    "        # 建立邻接矩阵的逻辑\n",
    "        indices = []\n",
    "        size = 3\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                index = i * size + j\n",
    "                if j < size - 1:\n",
    "                    indices.append((index, index + 1))  # 右\n",
    "                if i < size - 1:\n",
    "                    indices.append((index, index + size))  # 下\n",
    "                if i < size - 1 and j < size - 1:\n",
    "                    indices.append((index, index + size + 1))  # 右下角\n",
    "                if i < size - 1 and j > 0:\n",
    "                    indices.append((index, index + size - 1))  # 左下角\n",
    "        return torch.tensor(indices).t().contiguous()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 假设 x 的初始形状是 (9, 3, 3)，我们需要将其转换为 (9, 9)\n",
    "        x = x.view(9, -1)  # 将每个3x3的矩阵拉平成一个长度为9的向量\n",
    "        x = F.relu(self.conv1(x, self.edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.relu(self.conv2(x, self.edge_index))\n",
    "        x = x.view(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型和优化器\n",
    "model = GATClassifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.long()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.long()\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, dim=0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Training Loss: 0.8648, Test Accuracy: 0.7556\n",
      "Epoch: 2/50, Training Loss: 1.0231, Test Accuracy: 0.5547\n",
      "Epoch: 3/50, Training Loss: 0.8310, Test Accuracy: 0.5547\n",
      "Epoch: 4/50, Training Loss: 0.7845, Test Accuracy: 0.5547\n",
      "Epoch: 5/50, Training Loss: 0.7538, Test Accuracy: 0.5547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(model, test_loader)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[74], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 10\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\14390\\.conda\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_acc = test(model, test_loader)\n",
    "    print(f'Epoch: {epoch}/{num_epochs}, Training Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
